import numpy as np
from typing import List, Dict, Set, Tuple
import nltk
from nltk.corpus import stopwords
from sentence_transformers import SentenceTransformer, util
from keybert import KeyBERT

# --- åˆå§‹åŒ–åŠ è½½åŒº ---
try:
    STOP_WORDS = list(stopwords.words('english'))
except LookupError:
    nltk.download('stopwords')
    STOP_WORDS = list(stopwords.words('english'))

try:
    nltk.data.find("tokenizers/punkt")
except LookupError:
    nltk.download("punkt")
    nltk.download("punkt_tab")

print("ğŸ”„ æ­£åœ¨åŠ è½½ Embedding æ¨¡å‹ (all-MiniLM-L6-v2)...")
SENTENCE_MODEL = SentenceTransformer('all-MiniLM-L6-v2')
KEYBERT_MODEL = KeyBERT(model=SENTENCE_MODEL)
print("âœ… æ¨¡å‹åŠ è½½å®Œæ¯•ï¼\n")

# å¸¸ç”¨é¢œè‰² ANSI è½¬ä¹‰ç 
RED = "\033[31m"
GREEN = "\033[32m"
YELLOW = "\033[33m"
BLUE = "\033[34m"
RESET = "\033[0m"  # é‡ç½®é¢œè‰²
# --- --- ---

class ToolSemanticProcessor:
    def __init__(self,
                 tools: List[Dict[str, str]],
                 target_tool_name: str,
                 keybert_top_n: int = 10,
                 keyphrase_ngram_range: Tuple[int, int] = (1, 3),
                 conflict_threshold: float = 0.70): # é˜ˆå€¼ï¼šè¶Šä½è¶Šä¸¥æ ¼ï¼Œè¶Šé«˜è¶Šå®½æ¾

        if not any(tool['name'] == target_tool_name for tool in tools):
            raise ValueError(f"é”™è¯¯: ç›®æ ‡å·¥å…· '{target_tool_name}' ä¸åœ¨å·¥å…·åˆ—è¡¨ä¸­ã€‚")
            
        self.tools_data = {tool['name']: {'description': tool['description']} for tool in tools}
        self.target_tool_name = target_tool_name
        self.non_target_tool_names = [name for name in self.tools_data if name != target_tool_name]

        self.keybert_top_n = keybert_top_n
        self.keyphrase_ngram_range = keyphrase_ngram_range
        self.conflict_threshold = conflict_threshold

        self.doc_embeddings = {} 
        for name, data in self.tools_data.items():
            self.doc_embeddings[name] = SENTENCE_MODEL.encode(data['description'], convert_to_tensor=True)

    def _extract_keyphrases(self):
        print("--- æ­¥éª¤ 1: æå–æ‰€æœ‰å·¥å…·çš„å…³é”®çŸ­è¯­ ---")
        for name, data in self.tools_data.items():
            # KeyBERT æå–
            phrases = KEYBERT_MODEL.extract_keywords(
                data['description'],
                keyphrase_ngram_range=self.keyphrase_ngram_range,
                stop_words=STOP_WORDS,
                use_mmr=True, # Max Marginal Relevance ä¿è¯å¤šæ ·æ€§
                diversity=0.3,
                top_n=self.keybert_top_n
            )
            # è¿‡æ»¤æ‰è¿‡çŸ­çš„è¯ (é•¿åº¦<=2)ï¼Œä¿ç•™ç”± n-gram äº§ç”Ÿçš„æœ‰æ„ä¹‰çŸ­è¯­
            data['phrases'] = [p for p, s in phrases if len(p) > 2]
            
            # é¢„è®¡ç®—è¯¥å·¥å…·æå–å‡ºçš„çŸ­è¯­çš„å‘é‡ (åŠ é€Ÿæ¯”å¯¹)
            if data['phrases']:
                data['phrase_embeddings'] = SENTENCE_MODEL.encode(data['phrases'], convert_to_tensor=True)
            else:
                data['phrase_embeddings'] = None
            
            # æ‰“å°é¢„è§ˆ
            print(f"  ğŸ”¹ [{name}] åˆæ­¥æå– ({len(data['phrases'])}ä¸ª): {data['phrases'][:3]}...")

    def _find_semantic_conflicts(self):
        print(f"\n--- æ­¥éª¤ 2: è¯­ä¹‰å†²çªæ‰«æ (ç›¸ä¼¼åº¦é˜ˆå€¼ > {self.conflict_threshold}) ---")
        
        target_data = self.tools_data[self.target_tool_name]
        if target_data['phrase_embeddings'] is None:
            return []

        conflicts = [] 

        # æ‰«ææ‰€æœ‰éç›®æ ‡å·¥å…·
        for other_name in self.non_target_tool_names:
            other_data = self.tools_data[other_name]
            if other_data['phrase_embeddings'] is None:
                continue

            # âš¡ çŸ©é˜µè®¡ç®—: TargetçŸ­è¯­ x OtherçŸ­è¯­
            similarity_matrix = util.cos_sim(target_data['phrase_embeddings'], other_data['phrase_embeddings'])

            # éå†çŸ©é˜µï¼Œæ‰¾å‡ºç›¸ä¼¼åº¦è¶…æ ‡çš„å¯¹å­
            for i, target_phrase in enumerate(target_data['phrases']):
                for j, other_phrase in enumerate(other_data['phrases']):
                    score = similarity_matrix[i][j].item()
                    
                    if score > self.conflict_threshold:
                        conflicts.append({
                            "target_phrase": target_phrase,
                            "competitor_phrase": other_phrase,
                            "competitor_tool": other_name,
                            "similarity": score
                        })

        if conflicts:
            print(f"  âš ï¸ å‘ç° {len(conflicts)} ç»„è¯­ä¹‰æ¥è¿‘çš„å†²çªã€‚")
        else:
            print("  âœ… æœªå‘ç°æ˜¾è‘—å†²çªã€‚")
            
        return conflicts

    def _calculate_relevance(self, phrase, tool_name):
        """è®¡ç®—ï¼šæŸä¸ªçŸ­è¯­ vs æŸä¸ªå·¥å…·æè¿° çš„è¯­ä¹‰å¥‘åˆåº¦"""
        phrase_emb = SENTENCE_MODEL.encode(phrase, convert_to_tensor=True)
        doc_emb = self.doc_embeddings[tool_name]
        return util.cos_sim(phrase_emb, doc_emb).item()

    def _resolve_conflicts(self, conflicts):
        print("\n--- æ­¥éª¤ 3: å†²çªæ™ºèƒ½è£å†³ ---")
        
        # è®°å½•å¾…åˆ é™¤åå• (Tool -> Set of phrases)
        removal_plan = {name: set() for name in self.tools_data}

        for c in conflicts:
            t_phrase = c['target_phrase']
            o_phrase = c['competitor_phrase']
            o_tool = c['competitor_tool']
            
            # è£åˆ¤è¿›åœºï¼šåˆ†åˆ«è®¡ç®—çŸ­è¯­å¯¹å„è‡ªå·¥å…·çš„å¥‘åˆåº¦
            score_target = self._calculate_relevance(t_phrase, self.target_tool_name)
            score_competitor = self._calculate_relevance(o_phrase, o_tool)

            print(f"âš”ï¸  å†²çª: Target['{t_phrase}'] vs {o_tool}['{o_phrase}'] (ç›¸ä¼¼åº¦: {c['similarity']:.2f})")
            
            # è°çš„åˆ†æ•°ä½ï¼Œè°å°±æ”¾å¼ƒè¿™ä¸ªè¯
            if score_target >= score_competitor:
                print(f"    ğŸ† ç›®æ ‡å·¥å…·èƒœå‡º ({score_target:.3f} vs {score_competitor:.3f})")
                print(f"    ğŸ—‘ï¸  ç§»é™¤ {o_tool} çš„ '{o_phrase}'")
                removal_plan[o_tool].add(o_phrase)
            else:
                print(f"    ğŸ›¡ï¸ ç«å“å·¥å…·èƒœå‡º ({score_competitor:.3f} vs {score_target:.3f})")
                print(f"    ğŸ—‘ï¸  ç§»é™¤ Target çš„ '{t_phrase}'")
                removal_plan[self.target_tool_name].add(t_phrase)

        # æ‰§è¡Œåˆ é™¤æ“ä½œ
        for tool_name, phrases_to_remove in removal_plan.items():
            original_list = self.tools_data[tool_name]['phrases']
            self.tools_data[tool_name]['final_phrases'] = [p for p in original_list if p not in phrases_to_remove]

    def process(self):
        self._extract_keyphrases()
        conflicts = self._find_semantic_conflicts()
        
        # å¦‚æœæœ‰å†²çªåˆ™è§£å†³ï¼Œæ²¡å†²çªåˆ™ç›´æ¥å¤åˆ¶
        if conflicts:
            self._resolve_conflicts(conflicts)
        else:
            for name, data in self.tools_data.items():
                data['final_phrases'] = data['phrases']

        # è¡¥å……é€»è¾‘ï¼šé‚£äº›è™½ç„¶æ²¡å·å…¥å†²çªï¼Œä½†è¿˜æ²¡ç”Ÿæˆ final_phrases å­—æ®µçš„å·¥å…·ï¼ˆæ¯”å¦‚æ²¡æœ‰å†²çªçš„å·¥å…·ï¼‰
        # éœ€è¦ç¡®ä¿å®ƒä»¬ä¹Ÿæœ‰æ•°æ®ï¼Œå¦åˆ™æ‰“å°ä¼šæŠ¥é”™
        for name, data in self.tools_data.items():
            if 'final_phrases' not in data:
                 data['final_phrases'] = [p for p in data['phrases'] if p not in self._get_removal_set(name, conflicts)]


        # âœ… æœ€ç»ˆï¼šæ‰“å°æ‰€æœ‰å·¥å…·çš„æ¸…å•
        print("\n" + "="*40)
        print("ğŸŒ å…¨å±€æœ€ç»ˆå…³é”®çŸ­è¯­æ¸…å• (Global Results)")
        print("="*40)
        
        final_results = {}
        for name, data in self.tools_data.items():
            # æ’åºè®©è¾“å‡ºå¥½çœ‹ç‚¹
            final_list = sorted(data.get('final_phrases', []))
            final_results[name] = final_list

            # æ ‡è®°æ˜¯å¦æ˜¯ç›®æ ‡å·¥å…·
            if name == self.target_tool_name:
                prefix = f"{GREEN}ğŸ¯ TARGET{RESET}"
                name_color = GREEN
            else:
                prefix = f"{BLUE}ğŸ”§ TOOL{RESET}"
                name_color = BLUE

            print(f"{prefix}: [{name_color}{name}{RESET}] - å…± {len(final_list)} ä¸ªçŸ­è¯­")

            if len(final_list) > 0:
                # æ¯è¡Œæ‰“å° 2 ä¸ªçŸ­è¯­ï¼Œæ˜¾å¾—ç´§å‡‘ä¸€ç‚¹
                for i in range(0, len(final_list), 2):
                    chunk = final_list[i:i+2]
                    print("   â€¢ " + "   |   â€¢ ".join(chunk))
            else:
                print(f"   {RED}(No Key Phrase!){RESET}")

            print(f"{YELLOW}" + "-" * 20 + f"{RESET}")
            
        return final_results

    def _get_removal_set(self, tool_name, conflicts):
        # è¾…åŠ©å‡½æ•°ï¼šä¸ºäº†å¤„ç†ä¸Šé¢é‚£ç§æ²¡è¿› resolve é€»è¾‘çš„è¾¹ç¼˜æƒ…å†µ
        # åœ¨å½“å‰é€»è¾‘æµé‡Œå…¶å®å·²ç»åœ¨ resolve é‡Œå¤„ç†äº†ï¼Œè¿™é‡Œæ˜¯ä¸ºäº†ä»£ç å¥å£®æ€§
        return set()
